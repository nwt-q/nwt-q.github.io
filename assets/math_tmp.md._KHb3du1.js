import{_ as t,c as a,o as i,a8 as n}from"./chunks/framework.DMZhn_gK.js";const h=JSON.parse('{"title":"","description":"","frontmatter":{},"headers":[],"relativePath":"math/tmp.md","filePath":"math/tmp.md","lastUpdated":1764639858000}'),o={name:"math/tmp.md"};function r(p,e,s,l,c,d){return i(),a("div",null,[...e[0]||(e[0]=[n("<p>Reading Comprehension Questions (Based on the SAP Framework Paper)</p><p>Directions:</p><blockquote><p>Read the following questions carefully and choose the best answer from the options provided. Refer to the content of the paper for accurate responses.</p></blockquote><p>Questions:</p><ol><li><p>What is the core problem that the SAP framework aims to solve?</p><p>A. Reducing the computational cost of pre-trained language model (PLM) fine-tuning</p><p>B. Mitigating private data leakage in Model-as-a-Service (MaaS) based Parameter-Efficient Fine-Tuning (PEFT)</p><p>C. Improving the accuracy of text generation tasks for large language models</p><p>D. Simplifying the deployment process of customized PLMs on cloud platforms</p></li><li><p>Which two key techniques are integrated into the SAP framework to achieve privacy preservation?</p><p>A. Split learning and differential privacy</p><p>B. Low-Rank Adaptation (LoRA) and fully homomorphic encryption</p><p>C. Text privatization and attention-based token pruning</p><p>D. Embedding inversion defense and attribute inference resistance</p></li><li><p>What is the purpose of the Contributing-Token-Identification (CTI) method proposed in the paper?</p><p>A. To identify the optimal split layer between the bottom and top models in split learning</p><p>B. To balance model utility degradation and privacy leakage by adjusting perturbation on important tokens</p><p>C. To accelerate the fine-tuning process by pruning irrelevant tokens</p><p>D. To enhance the robustness of PLMs against white-box attacks</p></li><li><p>For text generation tasks (e.g., question-answering on SQuAD), how does CTI determine token importance?</p><p>A. By analyzing term frequency-inverse document frequency (TF-IDF)</p><p>B. By calculating the statistical contribution of tokens to each category</p><p>C. By leveraging attention scores from the Multi-head Attention (MHA) mechanism</p><p>D. By measuring the Euclidean distance between token embeddings</p></li><li><p>Which datasets are used to evaluate the performance of the SAP framework on text classification tasks?</p><p>A. SQuAD, Financial Phrasebank (FP), and Blog</p><p>B. Stanford Sentiment Treebank (SST), FP, and Blog</p><p>C. SST, SQuAD, and Llama-3 benchmark</p><p>D. Blog, FP, and Roberta-Large dataset</p></li><li><p>Why do existing privacy-preserving fine-tuning methods (e.g., DP-Forward, SLDP-FT) struggle to balance utility and privacy?</p><p>A. They require excessive computational resources for model training</p><p>B. They ignore the importance of tokens and rely on expensive differential privacy (DP) noise</p><p>C. They are incompatible with Parameter-Efficient Fine-Tuning (PEFT) techniques like LoRA</p><p>D. They can only be applied to text classification tasks, not generation tasks</p></li><li><p>What is the threat model assumption for the SAP framework?</p><p>A. The service provider is malicious and actively tampers with the fine-tuning protocol</p><p>B. The service provider is honest but curious, attempting to infer private data from intermediate representations</p><p>C. The userâ€™s private dataset is already compromised by external attackers</p><p>D. The top model parameters are accessible to unauthorized third parties</p></li><li><p>On which dataset did the SAP framework achieve a 65% improvement in empirical privacy with only a 1% degradation in model performance?</p><p>A. Financial Phrasebank (FP)</p><p>B. Blog (topic classification dataset)</p><p>C. Stanford Sentiment Treebank (SST)</p><p>D. SQuAD (question-answering dataset)</p></li></ol>",5)])])}const u=t(o,[["render",r]]);export{h as __pageData,u as default};
